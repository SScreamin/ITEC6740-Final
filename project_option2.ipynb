{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "<h1>ITEC 6740 Final Project</h1>\r\n",
                "\r\n",
                "<h3>Company News Article Sentiment Analysis</h3>\r\n",
                "\r\n",
                "<p>Based off tutorial here: https://www.youtube.com/watch?v=o-zM8onpQZY</p>"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "# Import necessary libraries\r\n",
                "\r\n",
                "from urllib.request import urlopen, Request\r\n",
                "from bs4 import BeautifulSoup"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "source": [
                "# Finviz url where news articles are pulled from\r\n",
                "finviz_url = \"https://finviz.com/quote.ashx?t=\"\r\n",
                "\r\n",
                "# Tickers appended to the url each corresponding to an organization\r\n",
                "tickers =[\"AMD\", \"NVDA\", \"INTC\"]\r\n",
                "\r\n",
                "# All the news articles are located in a table with id 'news-table'\r\n",
                "# Store this information in a dictionary\r\n",
                "news_tables = {}\r\n",
                "\r\n",
                "# loop through the tickers, construct the urls, pull the news tables\r\n",
                "for ticker in tickers:\r\n",
                "    # construct the url and create the request\r\n",
                "    url = finviz_url + ticker\r\n",
                "    req = Request(url=url, headers={'user-agent': 'my-app'})\r\n",
                "    # open the request\r\n",
                "    response = urlopen(req)\r\n",
                "    \r\n",
                "    # parse the html response\r\n",
                "    html = BeautifulSoup(response, 'html')\r\n",
                "\r\n",
                "    # find the news table object\r\n",
                "    news_table = html.find(id=\"news-table\")\r\n",
                "    # store it in the dictionary with the appropriate ticker as the key\r\n",
                "    news_tables[ticker] = news_table"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "source": [
                "# Parse the tables to retrieve relevant information\r\n",
                "\r\n",
                "# List to store the parsed data\r\n",
                "parsed_data = []\r\n",
                "\r\n",
                "# Iterate through the dictionary of news tables\r\n",
                "for ticker, news_table in news_tables.items():\r\n",
                "    # Retrieve the text and date data from the news tables by parsing table rows\r\n",
                "    for row in news_table.findAll('tr'):\r\n",
                "        title = row.a.get_text()\r\n",
                "        # date has two parts: day and time separated by a space\r\n",
                "        date_data = row.td.text.split(\" \")\r\n",
                "\r\n",
                "        # If there is an occurence where there is only a time\r\n",
                "        if len(date_data) == 1:\r\n",
                "            time = date_data[0]\r\n",
                "        # Otherwise, there is both a date and time\r\n",
                "        else:\r\n",
                "            date = date_data[0]\r\n",
                "            time = date_data[1]\r\n",
                "\r\n",
                "        parsed_data.append([ticker, date, time, title])"
            ],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}